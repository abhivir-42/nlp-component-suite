# Default training configuration

# General training parameters
training:
  epochs: 100
  batch_size: 32
  shuffle: true
  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.001
    monitor: "val_loss"
    mode: "min"

# Optimizer configuration
optimizer:
  name: "adam"  # Options: sgd, adam, adamw
  learning_rate: 0.001
  weight_decay: 0.0001
  momentum: 0.9  # Only used for SGD
  beta1: 0.9     # Only used for Adam/AdamW
  beta2: 0.999   # Only used for Adam/AdamW

# Learning rate scheduler
lr_scheduler:
  name: "cosine_annealing"  # Options: step, cosine_annealing, reduce_on_plateau
  step_size: 30             # Only used for StepLR
  gamma: 0.1                # Factor by which the learning rate is reduced
  min_lr: 1.0e-6            # Minimum learning rate
  patience: 5               # Only used for ReduceLROnPlateau
  cooldown: 0               # Only used for ReduceLROnPlateau

# Data loading configuration
data:
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  num_workers: 4
  pin_memory: true

# Logging configuration
logging:
  log_interval: 10  # Log training stats every N batches
  eval_interval: 1  # Evaluate on validation set every N epochs
  save_interval: 5  # Save checkpoint every N epochs
  tensorboard: true
  mlflow: true

# Checkpoint configuration
checkpointing:
  save_best: true
  save_last: true
  save_interval_epochs: 10 